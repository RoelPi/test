---
title: "Test"
output: html_notebook
---

# Preparation
## Loading necessary packages
```{r, include=FALSE}
library(data.table)
library(ggplot2)
library(xlsx)
library(lubridate)
library(DataExplorer)
library(missRanger)
library(e1071)
library(knitr)

set.seed(19880303)
options(scipen = 999)
```

## Loading the data
```{r, include=FALSE}
meta <- data.table(read.xlsx('LCDataDictionary.xlsx', 1))
meta <- meta[,1:2]

dt <- fread('loan.csv', stringsAsFactors = T)
dt <- dt[,c('loan_amnt','funded_amnt','term','int_rate','grade','annual_inc','issue_d','dti','revol_bal','total_pymnt','loan_status')]
dt[,issue_d := dmy(paste0('01-',issue_d))] # make datetime out of month column
```


## Exploring the data

All columns appear to be of the proper type. Right now, I don't see a reason to make the column 'term' continuous.

```{r}
str(dt)
```

Some findings:
* The column 'funded_amt' seems to be closely matching the 'loan_amt' column.
* There are some missing values in the 'annual_inc' and 'dti' columns

```{r}
summary(dt)
plot_missing(dt)
```

There are several ways we can handle the missing values:
* Using the mean of the column
* Impute using the same distribution as the known values
* Omit the rows with missing values
* A machine learning approach

I preferchoosing the latter and impute by chaining random forests. The _missRanger_ package offers a very easy interface in order to achieve this.
However, given the size of the data set, let's just impute with the median (skewed data, no mean).

```{r}
# dt_imputed <- missRanger(dt,. ~ ., maxiter = 5, verbose = T, seed = 19880303, splitrule = 'extratrees', num.trees = 25)

hist(dt$dti, breaks = 50)
hist(dt$annual_inc, breaks = 50)

dt_imputed <- copy(dt)
dt_imputed[is.na(annual_inc), annual_inc := median(dt$annual_inc, na.rm = T)]
dt_imputed[is.na(dti), dti := median(dt$dti, na.rm = T)]

plot_missing(dt_imputed)

dt <- copy(dt_imputed)
rm(dt_imputed)
```

# Exploratory Data Analysis
## Distribution of the data

To speed up the exploratory data analysis I have taken a sample of 250k objects.
```{r}
dt_eda <- dt[sample(.N,250000)]
```

Key findings:
* The 'loan_amt' and 'funded_amt' seem to have heavy right tails. There also appears to be a bias to rounded numbers. 
* The variables 'dti' and 'annual_inc' seem to reflect inequalities that you expect from distributions regarding income.
* 'int_rate' is _somewhat_ normally distributed, but has fat tails (leptokurtic).

```{r}
plot_histogram(dt_eda)

jpeg("eda_plot1.jpg", width = 1600, height = 900)
plot_histogram(dt_eda)
dev.off()

qqnorm(dt_eda$int_rate)
jpeg("eda_plot2.jpg", width = 1600, height = 900)
qqnorm(dt_eda$int_rate)
dev.off()

kurtosis(dt_eda$int_rate)
```

## Multicolinearity within the data
In this chunk I look at the correlation between the variables.

Findings:
* As I discussed previously, 'loan_amt' and 'funded_amt' are closely related. Given the context of this data set, total_paymnt and revol_bal too.
* The 'grade' is closely related to 'int_rate'.

```{r}
plot_correlation(dt[,colnames(dt_eda)[grepl('numeric|integer',sapply(dt_eda,class))],with=F])
# plot_correlation(dt[,-c('issue_d')])
jpeg("eda_plot3.jpg", width = 1600, height = 900)
plot_correlation(dt[,-c('issue_d')])
dev.off()

rm(dt_eda)
```


# Exploratory Data Analysis
First, let's get rid of loans we haven't tracked for 36 months in the data set.
This leaves us with 890k loans.

```{r}
dt[,term := NULL] # assume all loan periods to be 36 months
max_date <- max(dt$issue_d) - months(36)
sum(dt$issue_d <= max_date)
dt2 <- dt[dt$issue_d <= max_date,]
```

## What percentage of loans has been fully paid?
75.6% of loans has been fully paid.

```{r}
unique(dt2$loan_status)
print(dt2[,.(count = .N/nrow(dt2) * 100), by=.(loan_status)])
```

First, I set everything that is not fully paid to "Default".
Next, I created a 'year' column.

I aggregated the table by year and bucket, counting the amount of rows as value.
Then, I pivoted the data, with 'grade' and 'year' as rows, status as columns.
Finally, I calculated the share of defaults.

As you can see, loans rated G from 2008 performed the worst, but this is a really small cohort.
I list the flop 10.

```{r}
dt3 <- copy(dt2)

dt3[,status := 'Fully Paid']
dt3[loan_status != 'Fully Paid', status := 'Default']
dt3[,year := year(issue_d)]
dt3 <- dcast.data.table(dt3[,.(count = .N), by =.(grade, year, status)], 'grade + year ~ status')
dt3[,share_default := round(Default / (`Default` + `Fully Paid`),2)]
kable(head(dt3[order(-share_default)],10))
rm(dt3)
```

In the following chunk I list the top 10 of year/grades with the highest annualized rate of return.

```{r}
dt4 <- copy(dt2)
dt4 <- dt4[,.(count = .N, total_payment = sum(total_pymnt), funded_amount = sum(funded_amnt)), by =.(grade, year, status)]
dt4[,aror := (total_payment / funded_amount) ^ (1/3) - 1]
kable(head(dt4[order(-aror)],10))
rm(dt4)
```